{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Dihg1_EIpS0m",
      "metadata": {
        "id": "Dihg1_EIpS0m"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "144297de",
      "metadata": {
        "id": "144297de"
      },
      "outputs": [],
      "source": [
        "# adapted from: Yasser H: Breast cancer logistic regression\n",
        "# import necessary libraries, visualize spreadsheet in python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PNrJTEYLmQW9",
      "metadata": {
        "id": "PNrJTEYLmQW9"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Find location of notebook to set correct environment\n",
        "matches = glob.glob(f\"/content/drive/My Drive/**/ASBME-breast-cancer-logistic-regression-key.ipynb\", recursive=True)\n",
        "parent = os.path.abspath(os.path.join(matches[0], os.pardir))\n",
        "print(parent)\n",
        "os.chdir(parent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80bb7521",
      "metadata": {
        "id": "80bb7521"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"./breast-cancer.csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305bbc00",
      "metadata": {
        "id": "305bbc00"
      },
      "outputs": [],
      "source": [
        "num_features = len(df.columns[2:])\n",
        "print(f'Number of Features: {num_features}')\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_1 = df['texture_mean']\n",
        "feature_2 = df['radius_mean']\n",
        "\n",
        "plt.scatter(feature_1, feature_2, c=df['malignant'], cmap='bwr')\n",
        "plt.xlabel('texture_mean')\n",
        "plt.ylabel('radius_mean')\n",
        "plt.title('Scatter Plot visualization of two breast cancer measurements')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d-84qgglCIKn"
      },
      "id": "d-84qgglCIKn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# Pairplot for first few features (adjust based on dataset size)\n",
        "sns.pairplot(df.iloc[:, 1:5], hue='malignant', diag_kind='kde')\n",
        "plt.show()\n",
        "\n",
        "diagnoses = df['malignant']\n",
        "feature_of_interest = df['radius_mean']\n",
        "# sns.pairplot(diagnoses, hue='diagnosis')"
      ],
      "metadata": {
        "id": "6d4Aja1PtGj7"
      },
      "id": "6d4Aja1PtGj7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sci-Kit Learn Logistic Regression"
      ],
      "metadata": {
        "id": "6xTs2KKnI6rE"
      },
      "id": "6xTs2KKnI6rE"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = df.iloc[:, 2:], df['malignant']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(random_state=0, max_iter=5000).fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "print('accuracy: ', clf.score(X_test, y_test))\n",
        "\n",
        "# print(clf.predict_proba(X_test))\n",
        "# plt.bar(df.columns[2:],clf.coef_[0])\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "MN60HIO1FLoG"
      },
      "id": "MN60HIO1FLoG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ad8ec1dc",
      "metadata": {
        "id": "ad8ec1dc"
      },
      "source": [
        "## Feature Engineering (Min-max Scaling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285b5e02",
      "metadata": {
        "id": "285b5e02"
      },
      "outputs": [],
      "source": [
        "# Minmax Scaling on Features\n",
        "\n",
        "def MinMax(feature_data):\n",
        "    \"\"\"\n",
        "    feature_data is a vector of data for a specific feature\n",
        "    Data will range from 0-1\n",
        "    \"\"\"\n",
        "\n",
        "    min_val = min(feature_data)\n",
        "    max_val = max(feature_data)\n",
        "    range = max_val - min_val\n",
        "\n",
        "    # epsilon prevents divide by 0 error\n",
        "    epsilon = 10**(-8)\n",
        "\n",
        "    return (feature_data - min_val)/(range + epsilon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943b15b1",
      "metadata": {
        "id": "943b15b1"
      },
      "outputs": [],
      "source": [
        "# iloc selects all rows and only columns starting from 2nd index ('diagnosis',...)\n",
        "# apply columnwise\n",
        "\n",
        "df_scaled = df.copy()\n",
        "df_scaled.iloc[:, 2:] = df.iloc[:, 2:].apply(MinMax, axis = 0)\n",
        "\n",
        "# Check\n",
        "df_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a01ea4a",
      "metadata": {
        "id": "4a01ea4a"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa43e2db",
      "metadata": {
        "id": "fa43e2db"
      },
      "source": [
        "Obtain $y_{true}$ which is the vector of 0s and 1s corresponding to benign and malignant diagnosis' respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52a97ca",
      "metadata": {
        "id": "b52a97ca"
      },
      "outputs": [],
      "source": [
        "y_true = df_scaled['malignant']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7572e2",
      "metadata": {
        "id": "7a7572e2"
      },
      "outputs": [],
      "source": [
        "# Check ground truth y_true with diagnoses\n",
        "print(y_true.shape)\n",
        "\n",
        "print(y_true[15:22])\n",
        "print(df_scaled.malignant[15:22])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce99452c",
      "metadata": {
        "id": "ce99452c"
      },
      "source": [
        "Obtain $x$ which is the input dataset. We need to remove unnecessary columns from the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3041bb0e",
      "metadata": {
        "id": "3041bb0e"
      },
      "outputs": [],
      "source": [
        "# Drop the id and diagnosis columns\n",
        "# Obtain input dataset\n",
        "df_input = df_scaled.drop(df_scaled.columns[:2], axis = 'columns')\n",
        "\n",
        "# Check\n",
        "df_input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b5055a",
      "metadata": {
        "id": "70b5055a"
      },
      "source": [
        "Split the dataset into training and validation sets.\n",
        "- 70% training\n",
        "- 30% validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82349c5f",
      "metadata": {
        "id": "82349c5f"
      },
      "outputs": [],
      "source": [
        "# Training and Validation Datasets\n",
        "# 70% training\n",
        "# 30% validation\n",
        "\n",
        "N = len(df_input.radius_mean)\n",
        "print(f'Total Number of Patients: {N}')\n",
        "\n",
        "N_train = np.floor(N * 0.7).astype(int)\n",
        "N_valid = N - N_train\n",
        "print(f'Training Dataset Size: {N_train}')\n",
        "print(f'Validation Dataset Size: {N_valid}')\n",
        "\n",
        "# Split the dataset\n",
        "df_train = df_input[:N_train]\n",
        "df_valid = df_input[N_train:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f643969",
      "metadata": {
        "id": "3f643969"
      },
      "source": [
        "Objective function is the negative log likelihood. We are doing MLE - we aim to find the values of the weights that minimise the objective function. Optimisiation done via gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ef4e84",
      "metadata": {
        "id": "68ef4e84"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "# No bias term\n",
        "# Can add a bias term by adding a feature into x that is always = 1\n",
        "# Note, we are using mean NLL\n",
        "def negative_log_likelihood(weights, x, y_true):\n",
        "    z = np.dot(x, weights)\n",
        "    phi = y_true * np.log(sigmoid(z)) + (1 - y_true) * (1 - np.log(sigmoid(z)))\n",
        "    return -np.mean(phi)\n",
        "\n",
        "def gradient(weights, x, y_true):\n",
        "    z = np.dot(x, weights)\n",
        "    sig_z = sigmoid(z)\n",
        "    grad = np.dot(x.T, (sig_z - y_true)) / len(y_true)\n",
        "    return grad\n",
        "\n",
        "def sgd(x, y_true, weights, learning_rate, epochs):\n",
        "    loss_list = []\n",
        "    for epoch in range(epochs):\n",
        "        grad = gradient(weights, x, y_true)\n",
        "\n",
        "        weights -= learning_rate * grad\n",
        "        if epoch % 50 == 0:\n",
        "            loss = negative_log_likelihood(weights, x, y_true)\n",
        "            loss_list.append(loss)\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss:.4f}')\n",
        "    return weights, loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee20d6b2",
      "metadata": {
        "id": "ee20d6b2"
      },
      "outputs": [],
      "source": [
        "# Usually learning rate < 1\n",
        "# Works here but not good practise\n",
        "learning_rate = 1.5\n",
        "epochs = 2500\n",
        "init_weights = np.ones(num_features)\n",
        "x = df_train.values\n",
        "y_true_train = y_true[:N_train]\n",
        "optimal_weights, loss_list = sgd(x, y_true_train, init_weights, learning_rate, epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77cf4f61",
      "metadata": {
        "id": "77cf4f61"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(np.arange(len(loss_list)), loss_list)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Negative Log Likelihood (Loss)')\n",
        "plt.title('Learning Curve')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae31c02a",
      "metadata": {
        "id": "ae31c02a"
      },
      "source": [
        "\n",
        "Now we have obtained the optimal values of the weights, we need to check how accurate it is using the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950e6a2f",
      "metadata": {
        "id": "950e6a2f"
      },
      "outputs": [],
      "source": [
        "## Validation\n",
        "def predict(x, weights):\n",
        "    z = np.dot(x, weights)\n",
        "    y_pred = np.where(z > 0.5, 1, 0)\n",
        "    return y_pred\n",
        "\n",
        "x_valid = df_valid.values\n",
        "y_true_valid = y_true[N_train:]\n",
        "y_pred = predict(x_valid, optimal_weights)\n",
        "\n",
        "# number of correct pred / total number of pred * 100\n",
        "acc = np.mean(y_pred == y_true_valid) * 100\n",
        "print(f'{acc}% accuracy in validation set')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "778d0443",
      "metadata": {
        "id": "778d0443"
      },
      "outputs": [],
      "source": [
        "# Sanity Check\n",
        "\n",
        "print(f' Predicted: \\n {y_pred[:20]}')\n",
        "print(f' True: \\n {y_true_valid[:20].tolist()}')\n",
        "\n",
        "# 1 = malignant\n",
        "# 0 = benign"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 1829286,
          "sourceId": 2984728,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 5.687873,
      "end_time": "2025-03-01T23:56:42.981044",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-03-01T23:56:37.293171",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}